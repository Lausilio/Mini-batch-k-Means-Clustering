{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e532e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_TYPE ='local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f2cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# if Spark is run either in Local of Single-Container mode\n",
    "if CLUSTER_TYPE in ['local', 'docker_container']:\n",
    "    \n",
    "    # build a SparkSession \n",
    "    #   connect to the master node (address `localhost`) and the port where the master node is listening (7077)\n",
    "    #   declare the app name \n",
    "    #   either connect or create a new context\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://localhost:7077\")\\\n",
    "        .appName(\"First spark application\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# if Spark is run as Docker Container cluster (with docker-compose)\n",
    "elif CLUSTER_TYPE == 'docker_cluster':\n",
    "    \n",
    "    # build a SparkSession \n",
    "    #   connect to the master node (address `spark-master`) and the port where the master node is listening (7077)\n",
    "    #   declare the app name \n",
    "    #   configure the executor memory to 512 MB\n",
    "    #   either connect or create a new context\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://spark-master:7077\")\\\n",
    "        .appName(\"First spark application\")\\\n",
    "        .config(\"spark.executor.memory\", \"512m\")\\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    print(\"Variable CLUSTER_TYPE is not set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27b619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf3aae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SparkContext is created by default with the variable name sc\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c336488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark import SparkContext\n",
    "#from pyspark.ml.clustering import KMeans\n",
    "#from pyspark.ml.feature import VectorAssembler\n",
    "#from pyspark.sql import SQLContext\n",
    "\n",
    "#print (pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b84d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running spark default settings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb5b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate input data to work with\n",
    "\n",
    "#using scikit-learn method to generate N (=10) datasets of dimensionality (d=3)\n",
    "n_samples=100000\n",
    "N=10\n",
    "dim=3\n",
    "X, y = make_blobs(n_samples=n_samples, centers=N, n_features=dim, cluster_std=default, shuffle=default, random_state=42)\n",
    "\n",
    "#add id column string for recognition\n",
    "pddf = pd.DataFrame(X, columns=['x', 'y', 'z'])\n",
    "pddf['id'] = 'row'+pddf.index.astype(str)\n",
    "\n",
    "cols = list(pddf)\n",
    "cols.insert(0, cols.pop(cols.index('id')))\n",
    "pddf = pddf.ix[:, cols]\n",
    "pddf.head()\n",
    "\n",
    "#write array of data in .csv file\n",
    "pddf.to_csv('input.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac75489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#want to create an SQLContext, why?\n",
    "#Spark SQL is a  module for structured data processing differs from the basic Spark RDD API because \n",
    "#the interfaces provided gives more information about the structure of both the data and computation\n",
    "sqlContext = SQLContext(sc)\n",
    "#read data from csv to spark dataframe\n",
    "FEATURES_COL = ['x', 'y', 'z']\n",
    "path = 'input.csv'\n",
    "df = sqlContext.read.csv(path, header=True)\n",
    "df.show()\n",
    "#convert data to column of float\n",
    "for col in df.columns:\n",
    "    if col in FEATURES_COL:\n",
    "        df = df.withColumn(col,df[col].cast('float'))\n",
    "df = df.na.drop()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808d6997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature columns in clustering\n",
    "#store all features as an array of floats stored as a column (features)\n",
    "vecAssembler = VectorAssembler(inputCols=FEATURES_COL, outputCol=\"features\")\n",
    "df_kmeans = vecAssembler.transform(df).select('id', 'features')\n",
    "df_kmeans.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20be64c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimize k choice over a fraction of data\n",
    "cost = np.zeros(30)\n",
    "for k in range(2,30):\n",
    "    kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "    model = kmeans.fit(df_kmeans.sample(False,0.1, seed=42))\n",
    "    cost[k] = model.computeCost(df_kmeans)\n",
    "fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "ax.plot(range(2,20),cost[2:20])\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278be0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 12\n",
    "kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "model = kmeans.fit(df_kmeans)\n",
    "centers = model.clusterCenters()\n",
    "\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "    \n",
    "#assigning the individual rows to the nearest cluster centroid\n",
    "transformed = model.transform(df_kmeans).select('id', 'prediction')\n",
    "rows = transformed.collect()\n",
    "print(rows[:3])\n",
    "\n",
    "#return SQL database\n",
    "df_pred = sqlContext.createDataFrame(rows)\n",
    "df_pred.show()\n",
    "\n",
    "#join prdiction db and original db\n",
    "df_pred = df_pred.join(df, 'id')\n",
    "df_pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd8b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#original visualization\n",
    "D3 = plt.figure(figsize=(12,10)).gca(projection='D3')\n",
    "D3.scatter(X[:,0], X[:,1], X[:,2], c=y)\n",
    "D3.set_xlabel('x')\n",
    "D3.set_ylabel('y')\n",
    "D3.set_zlabel('z')\n",
    "plt.show()\n",
    "\n",
    "#cluster visualization\n",
    "pddf_pred = df_pred.toPandas().set_index('id')\n",
    "pddf_pred.head()\n",
    "D3 = plt.figure(figsize=(12,10)).gca(projection='D3')\n",
    "D3.scatter(pddf_pred.x, pddf_pred.y, pddf_pred.z, c=pddf_pred.prediction)\n",
    "D3.set_xlabel('x')\n",
    "D3.set_ylabel('y')\n",
    "D3.set_zlabel('z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7f4dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
