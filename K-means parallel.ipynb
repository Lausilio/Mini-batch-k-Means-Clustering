{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f255099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_TYPE ='local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e5b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# if Spark is run either in Local of Single-Container mode\n",
    "if CLUSTER_TYPE in ['local', 'docker_container']:\n",
    "    \n",
    "    # build a SparkSession \n",
    "    #   connect to the master node (address `localhost`) and the port where the master node is listening (7077)\n",
    "    #   declare the app name \n",
    "    #   either connect or create a new context\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://localhost:7077\")\\\n",
    "        .appName(\"Clustering using K-Means\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# if Spark is run as Docker Container cluster (with docker-compose)\n",
    "elif CLUSTER_TYPE == 'docker_cluster':\n",
    "    \n",
    "    # build a SparkSession \n",
    "    #   connect to the master node (address `spark-master`) and the port where the master node is listening (7077)\n",
    "    #   declare the app name \n",
    "    #   configure the executor memory to 512 MB\n",
    "    #   either connect or create a new context\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://spark-master:7077\")\\\n",
    "        .appName(\"Clustering using K-Means\")\\\n",
    "        .config(\"spark.executor.memory\", \"512m\")\\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    print(\"Variable CLUSTER_TYPE is not set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e3302",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a957d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SparkContext is created by default with the variable name sc\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b9388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running spark default settings\n",
    "from pyspark import SparkContext\n",
    "#from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "\n",
    "#print (pyspark.__version__)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import make_blobs\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate input data to work with\n",
    "\n",
    "#using scikit-learn method to generate N (=10) datasets of dimensionality (d=3)\n",
    "n_samples=100000\n",
    "N=10\n",
    "dim=3\n",
    "X, y = make_blobs(n_samples=n_samples, centers=N, n_features=dim, cluster_std=default, shuffle=default, random_state=25)\n",
    "\n",
    "#add id column string for recognition\n",
    "pddf = pd.DataFrame(X, columns=['x', 'y', 'z'])\n",
    "pddf['id'] = 'row'+pddf.index.astype(str)\n",
    "\n",
    "cols = list(pddf)\n",
    "cols.insert(0, cols.pop(cols.index('id')))\n",
    "pddf = pddf.ix[:, cols]\n",
    "pddf.head()\n",
    "\n",
    "#write array of data in .csv file\n",
    "pddf.to_csv('input.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9741c08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#want to create an SQLContext, why?\n",
    "#Spark SQL is a  module for structured data processing differs from the basic Spark RDD API because \n",
    "#the interfaces provided gives more information about the structure of both the data and computation\n",
    "sqlContext = SQLContext(sc)\n",
    "#read data from csv to spark dataframe\n",
    "FEATURES_COL = ['x', 'y', 'z']\n",
    "path = 'input.csv'\n",
    "df = sqlContext.read.csv(path, header=True)\n",
    "df.show()\n",
    "#convert data to column of float\n",
    "for col in df.columns:\n",
    "    if col in FEATURES_COL:\n",
    "        df = df.withColumn(col,df[col].cast('float'))\n",
    "df = df.na.drop()\n",
    "df.show()\n",
    "#feature columns in clustering\n",
    "#store all features as an array of floats stored as a column (features)\n",
    "vecAssembler = VectorAssembler(inputCols=FEATURES_COL, outputCol=\"features\")\n",
    "df_kmeans = vecAssembler.transform(df).select('id', 'features')\n",
    "df_kmeans.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize the data to bring them to a comparable scale\n",
    "\n",
    "#scale=StandardScaler(inputCol='features',outputCol='standardized')\n",
    "#data_scale=scale.fit(df_kmeans)\n",
    "#data_scale_output=data_scale.transform(df_kmeans)\n",
    "#data_scale_output.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf305d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimize k choice over a fraction of data\n",
    "cost = np.zeros(30)\n",
    "for k in range(2,30):\n",
    "    kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "    model = kmeans.fit(df_kmeans.sample(False,0.1, seed=12345))\n",
    "    cost[k] = model.computeCost(df_kmeans)\n",
    "fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "ax.plot(range(2,30),cost[2:30])\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb44ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.mllib includes a variant of the k-means++ called kmeans|| (parallel)\n",
    "#data parallelism creates parallelism by partitioning the dataset into smaller partitions,\n",
    "#result parallelism is based on targeted clusters\n",
    "\n",
    "\n",
    "# Load and parse the data\n",
    "\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(parsedData, N, maxIterations=default, initializationMode=\"k-means||\",seed=default,initializationSteps=default,\n",
    "                       epsilon=default, initialModel=\"KMeansModel\") #kMeansModel to change\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
    "\n",
    "# Save and load model\n",
    "clusters.save(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")\n",
    "sameModel = KMeansModel.load(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f2ad93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
