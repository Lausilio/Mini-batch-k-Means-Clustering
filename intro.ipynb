{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "529c779f",
   "metadata": {},
   "source": [
    "# MAPD_B Final project\n",
    "\n",
    "Group 7\n",
    "\n",
    "*George P.Prodan*\n",
    "\n",
    "*Walter Martemucci*\n",
    "\n",
    "*Lorenzo Ausilio*\n",
    "\n",
    "*Farshad Jafarpour*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0389869",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The k-means optimization problem is to find the set C of cluster centers c ∈ R m, with |C| = k, to minimize over a set\n",
    "\n",
    "X of examples x ∈ R m the following objective function:\n",
    "\n",
    "min X\n",
    "x∈X\n",
    "\n",
    "$$||f(C, x) − x||^2$$\n",
    "\n",
    "Here, f(C, x) returns the nearest cluster center c ∈ C to x using Euclidean distance.\n",
    "\n",
    "In our project we harnessed mini-batch optimization for K-means clustering. The reason is that mini-batches have smaller stochastic noise than examples in SGD. The Algorithm for Mini batch K-means is:\n",
    "Algorithm 1 Mini-batch k-Means.\n",
    "\n",
    "\n",
    "$$c ← (1 − η)c + ηx $$\n",
    "<br>\n",
    "<ul>\n",
    "    <li> c - the cluster center\n",
    "    <li> η - learning rate\n",
    "    <li> x - sample\n",
    "</ul>\n",
    "\n",
    "In addition, we implemented a handy method called data parallelization.\n",
    " Data parallelism is a popular technique used to speed up training on large mini-batches when each mini-batch is too large to fit on a GPU. Under data parallelism, a mini-batch is split up into smaller sized batches that are small enough to fit on the memory available on different GPUs on the network.\n",
    "\n",
    "We use \"Spark\" as a cluster processing engine that allows data to be processed in parallel. Apache Spark's parallelism will enable developers to run tasks parallelly and independently on hundreds of computers in a cluster. All thanks to Apache Spark's fundamental idea, RDD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24279ad",
   "metadata": {},
   "source": [
    "# K-means parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36d7c19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_TYPE ='local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff25bf7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-060e03ad2370>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# if Spark is run either in Local of Single-Container mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mCLUSTER_TYPE\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'local'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'docker_container'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# if Spark is run either in Local of Single-Container mode\n",
    "if CLUSTER_TYPE in ['local', 'docker_container']:\n",
    "    \n",
    "    # build a SparkSession \n",
    "    #   connect to the master node (address `localhost`) and the port where the master node is listening (7077)\n",
    "    #   declare the app name \n",
    "    #   either connect or create a new context\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://localhost:7077\")\\\n",
    "        .appName(\"Clustering using K-Means\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# if Spark is run as Docker Container cluster (with docker-compose)\n",
    "elif CLUSTER_TYPE == 'docker_cluster':\n",
    "    \n",
    "    # build a SparkSession \n",
    "    #   connect to the master node (address `spark-master`) and the port where the master node is listening (7077)\n",
    "    #   declare the app name \n",
    "    #   configure the executor memory to 512 MB\n",
    "    #   either connect or create a new context\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://spark-master:7077\")\\\n",
    "        .appName(\"Clustering using K-Means\")\\\n",
    "        .config(\"spark.executor.memory\", \"512m\")\\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    print(\"Variable CLUSTER_TYPE is not set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc40033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SparkContext is created by default with the variable name sc\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ca0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running spark default settings\n",
    "from pyspark import SparkContext\n",
    "#from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "\n",
    "#print (pyspark.__version__)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import make_blobs\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba57fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate input data to work with\n",
    "\n",
    "#using scikit-learn method to generate N (=10) datasets of dimensionality (d=3)\n",
    "n_samples=100000\n",
    "N=10\n",
    "dim=3\n",
    "X, y = make_blobs(n_samples=n_samples, centers=N, n_features=dim, cluster_std=default, shuffle=default, random_state=25)\n",
    "\n",
    "#add id column string for recognition\n",
    "pddf = pd.DataFrame(X, columns=['x', 'y', 'z'])\n",
    "pddf['id'] = 'row'+pddf.index.astype(str)\n",
    "\n",
    "cols = list(pddf)\n",
    "cols.insert(0, cols.pop(cols.index('id')))\n",
    "pddf = pddf.ix[:, cols]\n",
    "pddf.head()\n",
    "\n",
    "#write array of data in .csv file\n",
    "pddf.to_csv('input.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdfd1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#want to create an SQLContext, why?\n",
    "#Spark SQL is a  module for structured data processing differs from the basic Spark RDD API because \n",
    "#the interfaces provided gives more information about the structure of both the data and computation\n",
    "sqlContext = SQLContext(sc)\n",
    "#read data from csv to spark dataframe\n",
    "FEATURES_COL = ['x', 'y', 'z']\n",
    "path = 'input.csv'\n",
    "df = sqlContext.read.csv(path, header=True)\n",
    "df.show()\n",
    "#convert data to column of float\n",
    "for col in df.columns:\n",
    "    if col in FEATURES_COL:\n",
    "        df = df.withColumn(col,df[col].cast('float'))\n",
    "df = df.na.drop()\n",
    "df.show()\n",
    "#feature columns in clustering\n",
    "#store all features as an array of floats stored as a column (features)\n",
    "vecAssembler = VectorAssembler(inputCols=FEATURES_COL, outputCol=\"features\")\n",
    "df_kmeans = vecAssembler.transform(df).select('id', 'features')\n",
    "df_kmeans.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29716c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize the data to bring them to a comparable scale\n",
    "\n",
    "#scale=StandardScaler(inputCol='features',outputCol='standardized')\n",
    "#data_scale=scale.fit(df_kmeans)\n",
    "#data_scale_output=data_scale.transform(df_kmeans)\n",
    "#data_scale_output.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f620539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimize k choice over a fraction of data\n",
    "cost = np.zeros(30)\n",
    "for k in range(2,30):\n",
    "    kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "    model = kmeans.fit(df_kmeans.sample(False,0.1, seed=12345))\n",
    "    cost[k] = model.computeCost(df_kmeans)\n",
    "fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "ax.plot(range(2,30),cost[2:30])\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060b9851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.mllib includes a variant of the k-means++ called kmeans|| (parallel)\n",
    "#data parallelism creates parallelism by partitioning the dataset into smaller partitions,\n",
    "#result parallelism is based on targeted clusters\n",
    "\n",
    "\n",
    "# Load and parse the data\n",
    "\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(parsedData, N, maxIterations=default, initializationMode=\"k-means||\",seed=default,initializationSteps=default,\n",
    "                       epsilon=default, initialModel=\"KMeansModel\") #kMeansModel to change\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
    "\n",
    "# Save and load model\n",
    "clusters.save(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")\n",
    "sameModel = KMeansModel.load(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41034650",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ea8f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_TYPE ='local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c643a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# if Spark is run either in Local of Single-Container mode\n",
    "if CLUSTER_TYPE in ['local', 'docker_container']:\n",
    "    \n",
    "    # build a SparkSession \n",
    "    #   connect to the master node (address `localhost`) and the port where the master node is listening (7077)\n",
    "    #   declare the app name \n",
    "    #   either connect or create a new context\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://localhost:7077\")\\\n",
    "        .appName(\"First spark application\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# if Spark is run as Docker Container cluster (with docker-compose)\n",
    "elif CLUSTER_TYPE == 'docker_cluster':\n",
    "    \n",
    "    # build a SparkSession \n",
    "    #   connect to the master node (address `spark-master`) and the port where the master node is listening (7077)\n",
    "    #   declare the app name \n",
    "    #   configure the executor memory to 512 MB\n",
    "    #   either connect or create a new context\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://spark-master:7077\")\\\n",
    "        .appName(\"First spark application\")\\\n",
    "        .config(\"spark.executor.memory\", \"512m\")\\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    print(\"Variable CLUSTER_TYPE is not set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fd9524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SparkContext is created by default with the variable name sc\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6df8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "#print (pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e40ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running spark default settings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75521488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate input data to work with\n",
    "\n",
    "#using scikit-learn method to generate N (=10) datasets of dimensionality (d=3)\n",
    "n_samples=100000\n",
    "N=10\n",
    "dim=3\n",
    "X, y = make_blobs(n_samples=n_samples, centers=N, n_features=dim, cluster_std=default, shuffle=default, random_state=25)\n",
    "\n",
    "#add id column string for recognition\n",
    "pddf = pd.DataFrame(X, columns=['x', 'y', 'z'])\n",
    "pddf['id'] = 'row'+pddf.index.astype(str)\n",
    "\n",
    "cols = list(pddf)\n",
    "cols.insert(0, cols.pop(cols.index('id')))\n",
    "pddf = pddf.ix[:, cols]\n",
    "pddf.head()\n",
    "\n",
    "#write array of data in .csv file\n",
    "pddf.to_csv('input.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccff2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#want to create an SQLContext, why?\n",
    "#Spark SQL is a  module for structured data processing differs from the basic Spark RDD API because \n",
    "#the interfaces provided gives more information about the structure of both the data and computation\n",
    "sqlContext = SQLContext(sc)\n",
    "#read data from csv to spark dataframe\n",
    "FEATURES_COL = ['x', 'y', 'z']\n",
    "path = 'input.csv'\n",
    "df = sqlContext.read.csv(path, header=True)\n",
    "df.show()\n",
    "#convert data to column of float\n",
    "for col in df.columns:\n",
    "    if col in FEATURES_COL:\n",
    "        df = df.withColumn(col,df[col].cast('float'))\n",
    "df = df.na.drop()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b50d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature columns in clustering\n",
    "#store all features as an array of floats stored as a column (features)\n",
    "vecAssembler = VectorAssembler(inputCols=FEATURES_COL, outputCol=\"features\")\n",
    "df_kmeans = vecAssembler.transform(df).select('id', 'features')\n",
    "df_kmeans.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01441bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimize k choice over a fraction of data\n",
    "cost = np.zeros(30)\n",
    "for k in range(2,30):\n",
    "    kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "    model = kmeans.fit(df_kmeans.sample(False,0.1, seed=12345))\n",
    "    cost[k] = model.computeCost(df_kmeans)\n",
    "fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "ax.plot(range(2,30),cost[2:30])\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26819658",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 12\n",
    "kmeans = KMeans().setK(k).setSeed(12345).setFeaturesCol(\"features\")\n",
    "model = kmeans.fit(df_kmeans)\n",
    "centers = model.clusterCenters()\n",
    "\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "    \n",
    "#assigning the individual rows to the nearest cluster centroid\n",
    "transformed = model.transform(df_kmeans).select('id', 'prediction')\n",
    "rows = transformed.collect()\n",
    "print(rows[:3])\n",
    "\n",
    "#return SQL database\n",
    "df_pred = sqlContext.createDataFrame(rows)\n",
    "df_pred.show()\n",
    "\n",
    "#join prdiction db and original db\n",
    "df_pred = df_pred.join(df, 'id')\n",
    "df_pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6c937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#original visualization\n",
    "D3 = plt.figure(figsize=(12,10)).gca(projection='D3')\n",
    "D3.scatter(X[:,0], X[:,1], X[:,2], c=y)\n",
    "D3.set_xlabel('x')\n",
    "D3.set_ylabel('y')\n",
    "D3.set_zlabel('z')\n",
    "plt.show()\n",
    "\n",
    "#cluster visualization\n",
    "pddf_pred = df_pred.toPandas().set_index('id')\n",
    "pddf_pred.head()\n",
    "D3 = plt.figure(figsize=(12,10)).gca(projection='D3')\n",
    "D3.scatter(pddf_pred.x, pddf_pred.y, pddf_pred.z, c=pddf_pred.prediction)\n",
    "D3.set_xlabel('x')\n",
    "D3.set_ylabel('y')\n",
    "D3.set_zlabel('z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f652d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae03f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3028a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31036768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5504103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e070da88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b00f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2715b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8226488e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a486cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
